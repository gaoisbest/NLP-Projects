{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %load train.py\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "from model import BrandsNERModel\n",
    "from conlleval import return_report\n",
    "from utils_train import BatchManager, prepare_dataset, update_tag_scheme\n",
    "from utils_io import clean, make_path, load_sentences, augment_with_pretrained, tag_mapping, char_mapping, load_config, save_config, load_word2vec\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "# input\n",
    "flags.DEFINE_string('data_dir',          'data',   'Path for training, development, testing and embedding data.')\n",
    "\n",
    "# output\n",
    "flags.DEFINE_string('summary_dir',  'summaries',   'Path for training and testing summaries.')\n",
    "flags.DEFINE_string('ckpt_dir',     'checkpoints',  'Path for saving model checkpoints.')\n",
    "\n",
    "# pre-processing\n",
    "flags.DEFINE_boolean('zeros',            True,       'Replace digits with zero.')\n",
    "flags.DEFINE_boolean('lower',            True,       'Convert character to lower case.')\n",
    "flags.DEFINE_string('tag_schema',     'iobes',    'Tagging schema iobes or iob')\n",
    "\n",
    "# bi-directional lstm + crf model\n",
    "flags.DEFINE_integer('word_dim',            20,        'Embedding dimension for word, 0 if not used.')\n",
    "flags.DEFINE_integer('char_dim',           100,        'Embedding dimension for character.')\n",
    "flags.DEFINE_integer('num_units',          100,        'Number of recurrent units in LSTM cell.')\n",
    "\n",
    "# training\n",
    "flags.DEFINE_float('learning_rate',       0.001,      'Initial learning rate.')\n",
    "flags.DEFINE_float('max_gradient_norm',       5,      'Clip gradients to this norm.')\n",
    "flags.DEFINE_float('batch_size',             20,      'Batch size to use during training.')\n",
    "flags.DEFINE_float('keep_prop',             0.5,      'Initial dropout rate.')\n",
    "flags.DEFINE_boolean('use_crf',            False,      'Use crf layer or softmax layer as the top layer.')\n",
    "flags.DEFINE_integer('num_epoch',           100,      'Number of epochs.')\n",
    "\n",
    "# util\n",
    "flags.DEFINE_boolean('clean',              True,      'Clean all the training-related folders and files.')\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "def create_model(session, Model_class, path, load_vec, config, id_to_char):\n",
    "    \"\"\"\n",
    "    Train the new model or re-use trained model.\n",
    "    \"\"\"\n",
    "    model = Model_class(config)\n",
    "    ckpt = tf.train.get_checkpoint_state(checkpoint_dir=path)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        print('Reading model parameters from %s' % ckpt.model_checkpoint_path)\n",
    "        model.saver.restore(session, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        print('Created model with fresh parameters.')\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        # assign character embeddings\n",
    "        emb_weights = session.run(model.char_embeddings.read_value())\n",
    "        emb_weights = load_vec(config['character_embedding_file'], id_to_char, config['char_dim'], emb_weights)\n",
    "        session.run(model.char_embeddings.assign(emb_weights))\n",
    "    return model\n",
    "\n",
    "\n",
    "def train(training_file_name, dev_file_name, test_file_name, maps_file_name, character_embedding_file_name, config_file_name):\n",
    "    \"\"\"\n",
    "    Train main entrance.\n",
    "    \"\"\"\n",
    "    training_file = os.path.join(FLAGS.data_dir, training_file_name)\n",
    "    dev_file = os.path.join(FLAGS.data_dir, dev_file_name)\n",
    "    test_file = os.path.join(FLAGS.data_dir, test_file_name)\n",
    "    maps_file = os.path.join(FLAGS.data_dir, maps_file_name)\n",
    "    embedding_file = os.path.join(FLAGS.data_dir, character_embedding_file_name)\n",
    "    config_file = os.path.join(FLAGS.data_dir, config_file_name)\n",
    "\n",
    "    # load data sets\n",
    "    # brands.train, dev, test tagging schema: IOB\n",
    "    # train_sentences format: [[['a', 'B-SHOE'], ['b', 'I-SHOE'], ['c', I-SHOE], ['d', 'O'], ...], [next training example data]]\n",
    "    train_sentences = load_sentences(training_file, FLAGS.zeros)\n",
    "    dev_sentences = load_sentences(dev_file, FLAGS.zeros)\n",
    "    test_sentences = load_sentences(test_file, FLAGS.zeros)\n",
    "\n",
    "    # Use selected tagging scheme (IOB / IOBES)\n",
    "    # train_sentences format: [[['a', 'B-SHOE'], ['b', 'I-SHOE'], ['c', E-SHOE], ['d', 'O'], ...], [next training example data]]\n",
    "    update_tag_scheme(train_sentences, FLAGS.tag_schema)\n",
    "    update_tag_scheme(test_sentences, FLAGS.tag_schema)\n",
    "\n",
    "    # create maps.pkl if not exist\n",
    "    # maps.pkl contains: char_to_id, id_to_char, tag_to_id, id_to_tag\n",
    "    if not os.path.isfile(maps_file):\n",
    "        print('create map file')\n",
    "        # create dictionary for each character\n",
    "        dict_chars_train = char_mapping(train_sentences, FLAGS.lower)[0]\n",
    "        # update dictionary by add the characters in embedding files or test data set\n",
    "        dict_chars, char_to_id, id_to_char = augment_with_pretrained(dict_chars_train.copy(), embedding_file, list(itertools.chain.from_iterable([[w[0] for w in s] for s in dev_sentences])))\n",
    "        # Create a dictionary and a mapping for tags\n",
    "        _t, tag_to_id, id_to_tag = tag_mapping(train_sentences)\n",
    "        # pickle data\n",
    "        with open(maps_file, 'wb') as f:\n",
    "            pickle.dump([char_to_id, id_to_char, tag_to_id, id_to_tag], f)\n",
    "    else:\n",
    "        print('load map file')\n",
    "        with open(maps_file, 'rb') as f:\n",
    "            char_to_id, id_to_char, tag_to_id, id_to_tag = pickle.load(f)\n",
    "\n",
    "    # convert character, tag, word segmentation to id\n",
    "    train_data = prepare_dataset(train_sentences, char_to_id, tag_to_id, FLAGS.lower)\n",
    "    dev_data = prepare_dataset(dev_sentences, char_to_id, tag_to_id, FLAGS.lower)\n",
    "    test_data = prepare_dataset(test_sentences, char_to_id, tag_to_id, FLAGS.lower)\n",
    "    print('%i / %i / %i sentences in train / dev / test.' % (len(train_data), len(dev_data), len(test_data)))\n",
    "\n",
    "    # prepare mini-batch data\n",
    "    train_manager = BatchManager(train_data, FLAGS.batch_size)\n",
    "    dev_manager = BatchManager(dev_data, 100)\n",
    "    test_manager = BatchManager(test_data, 100)\n",
    "\n",
    "    # make path for store summary and model if not exist\n",
    "    make_path(FLAGS)\n",
    "\n",
    "    if os.path.isfile(config_file):\n",
    "        config = load_config(config_file)\n",
    "    else:\n",
    "        config = OrderedDict()\n",
    "        config['num_chars'] = len(char_to_id)\n",
    "        config['char_dim'] = FLAGS.char_dim\n",
    "        config['num_tags'] = len(tag_to_id)\n",
    "        config['word_dim'] = FLAGS.word_dim\n",
    "        config['num_units'] = FLAGS.num_units\n",
    "        config['batch_size'] = FLAGS.batch_size\n",
    "        config['character_embedding_file'] = os.path.join(FLAGS.data_dir, character_embedding_file_name)\n",
    "        config['max_gradient_norm'] = FLAGS.max_gradient_norm\n",
    "        config['keep_prop'] = FLAGS.keep_prop\n",
    "        config['learning_rate'] = FLAGS.learning_rate\n",
    "        config['zeros'] = FLAGS.zeros\n",
    "        config['lower'] = FLAGS.lower\n",
    "        config['use_crf'] = FLAGS.use_crf\n",
    "        save_config(config, config_file)\n",
    "\n",
    "    # config parameters for the tf.Session\n",
    "    tf_config = tf.ConfigProto()\n",
    "    tf_config.gpu_options.allow_growth = True\n",
    "    # number of mini-batches per epoch\n",
    "    steps_per_epoch = train_manager.len_data\n",
    "\n",
    "    with tf.Session(config=tf_config) as sess:\n",
    "        # train_writer = tf.summary.FileWriter(logdir=os.path.join(FLAGS.summary_dir, 'train'), graph=sess.graph)\n",
    "        # test_writer = tf.summary.FileWriter(logdir=os.path.join(FLAGS.summary_dir, 'test'), graph=sess.graph)\n",
    "        train_writer = tf.summary.FileWriter(logdir=FLAGS.summary_dir, graph=sess.graph)\n",
    "        model = create_model(sess, BrandsNERModel, FLAGS.ckpt_dir, load_word2vec, config, id_to_char)\n",
    "        loss = []\n",
    "        for i in range(FLAGS.num_epoch):\n",
    "            for mini_batch in train_manager.iter_batch(shuffle=True):\n",
    "                global_step, mini_batch_cost, mini_batch_summary = model.step(sess, mini_batch, is_training=True, keep_prop=FLAGS.keep_prop)\n",
    "                train_writer.add_summary(summary=mini_batch_summary, global_step=global_step)\n",
    "                loss.append(mini_batch_cost)\n",
    "                if global_step % 100 == 0:\n",
    "                    print('iteration:{} step:{}/{}, NER loss:{:>9.6f}'.format(i+1, global_step%steps_per_epoch, steps_per_epoch, np.mean(loss)))\n",
    "                    loss = []\n",
    "            # evaluate the model on development data\n",
    "            best = evaluate(sess, model, 'dev', dev_manager, id_to_tag)\n",
    "            # if have better dev F1 score until now, then save the model\n",
    "            if best:\n",
    "                model.saver.save(sess=sess, save_path=os.path.join(FLAGS.ckpt_dir, 'Brands_ner.ckpt'), global_step=model.global_step.eval())\n",
    "            # report the test F1 score\n",
    "            evaluate(sess, model, 'test', test_manager, id_to_tag)\n",
    "\n",
    "\n",
    "def evaluate(sess, model, name, data, id_to_tag):\n",
    "    print('====================== evaluate:{}'.format(name))\n",
    "\n",
    "    # ner_results contains 'character - real tag - predicted tag' for all samples in 'data'\n",
    "    ner_results = model.evaluate(sess, data, id_to_tag)\n",
    "    eval_lines = test_ner(ner_results, FLAGS.data_dir)\n",
    "\n",
    "    for line in eval_lines:\n",
    "        print(line)\n",
    "    f1 = float(eval_lines[1].strip().split()[-1])\n",
    "\n",
    "    if name == 'dev':\n",
    "        best_test_f1 = model.best_dev_f1.eval()\n",
    "        if f1 > best_test_f1:\n",
    "            sess.run(model.best_dev_f1.assign(f1))\n",
    "            print('new best dev f1 score:{:>.3f}'.format(f1))\n",
    "        return f1 > best_test_f1\n",
    "    elif name == 'test':\n",
    "        best_test_f1 = model.best_test_f1.eval()\n",
    "        if f1 > best_test_f1:\n",
    "            sess.run(model.best_test_f1.assign(f1))\n",
    "            print('new best test f1 score:{:>.3f}'.format(f1))\n",
    "        return f1 > best_test_f1\n",
    "\n",
    "def test_ner(results, path):\n",
    "    \"\"\"\n",
    "    Report the performance.\n",
    "    \"\"\"\n",
    "    output_file = os.path.join(path, 'Brands_ner_predict.utf8')\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        to_write = []\n",
    "        for block in results:\n",
    "            for line in block:\n",
    "                to_write.append(line + '\\n')\n",
    "            to_write.append('\\n')\n",
    "\n",
    "        f.writelines(to_write)\n",
    "    eval_lines = return_report(output_file)\n",
    "    return eval_lines\n",
    "\n",
    "def main(_):\n",
    "    if FLAGS.clean:\n",
    "        clean(FLAGS, 'maps.pkl', 'BrandsNERModel.config', 'Brands_ner_predict.utf8')\n",
    "    train('brands.train', 'brands.dev', 'brands.test', 'maps.pkl', 'wiki_100.utf8', 'BrandsNERModel.config')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run(main)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
