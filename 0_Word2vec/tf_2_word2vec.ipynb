{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# revised version from https://github.com/chiphuyen/stanford-tensorflow-tutorials/blob/master/examples/04_word2vec_visualize.py\n",
    "# [1] change tf.nn.nce_loss to tf.nn.sampled_softmax_loss\n",
    "# [2] mkdir function\n",
    "# [3] AdagradOptimizer\n",
    "# [4] modify `generate_sample` function to infinite loop for each training article\n",
    "\n",
    "import codecs\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-Gram model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SkipGramModel:\n",
    "    \"\"\" Build the graph for word2vec model \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, batch_size, num_sampled, learning_rate):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_sampled = num_sampled\n",
    "        self.lr = learning_rate\n",
    "        # self.global_step is a counter, so it should not be trained (i.e., trainable=False)\n",
    "        self.global_step = tf.Variable(initial_value=0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "    def _create_placeholders(self):\n",
    "        \"\"\" Step 1: define the placeholders for input and output \"\"\"\n",
    "        with tf.name_scope('input_data'):\n",
    "            self.center_words = tf.placeholder(dtype=tf.int32, shape=[self.batch_size], name='center_words')\n",
    "            self.target_words = tf.placeholder(dtype=tf.int32, shape=[self.batch_size, 1], name='target_words')\n",
    "\n",
    "    def _create_embedding(self):\n",
    "        \"\"\" Step 2: define weights. In word2vec, it's actually the weights that we care about \"\"\"\n",
    "        with tf.name_scope('embedding'):\n",
    "            self.embed_matrix = tf.Variable(initial_value=tf.random_uniform(shape=[self.vocab_size, self.embed_size], minval=-1.0, maxval=1.0), name='embed_matrix')\n",
    "\n",
    "    def _create_loss(self):\n",
    "        \"\"\" Step 3 + 4: define the model + the loss function \"\"\"\n",
    "        with tf.name_scope('loss'):\n",
    "            # Step 3: define the inference\n",
    "            # directly get the embedding of 'ids'\n",
    "            # see https://stackoverflow.com/questions/34870614/what-does-tf-nn-embedding-lookup-function-do/41922877\n",
    "            embed = tf.nn.embedding_lookup(params=self.embed_matrix, ids=self.center_words, name='embed')\n",
    "\n",
    "            # Step 4: define loss function\n",
    "            # the results showed that sampled_softmax_loss is litter better than nce_loss\n",
    "            softmax_weights = tf.Variable(initial_value=tf.truncated_normal([self.vocab_size, self.embed_size], stddev=1.0 / self.embed_size ** 0.5), name='sampled_softmax_weight')\n",
    "            softmax_biases = tf.Variable(initial_value=tf.zeros([self.vocab_size]), name='sampled_softmax_bias')\n",
    "            self.loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=embed,\n",
    "                                                                  labels=self.target_words, num_sampled=NUM_SAMPLED, num_classes=self.vocab_size, name='sampled_softmax_loss'))\n",
    "            '''\n",
    "            # construct variables for NCE loss\n",
    "            nce_weight = tf.Variable(initial_value=tf.truncated_normal(shape=[self.vocab_size, self.embed_size], stddev=1.0 / (self.embed_size ** 0.5)), name='nce_weight')\n",
    "            nce_bias = tf.Variable(initial_value=tf.zeros([VOCAB_SIZE]), name='nce_bias')\n",
    "\n",
    "            # define loss function to be NCE loss function\n",
    "            self.loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weight,\n",
    "                                                      biases=nce_bias,\n",
    "                                                      labels=self.target_words,\n",
    "                                                      inputs=embed,\n",
    "                                                      num_sampled=self.num_sampled,\n",
    "                                                      num_classes=self.vocab_size), name='nce_loss')\n",
    "            '''\n",
    "\n",
    "\n",
    "    def _create_optimizer(self):\n",
    "        \"\"\" Step 5: define optimizer \"\"\"\n",
    "        # do not forget global_step parameter\n",
    "        self.optimizer = tf.train.AdagradOptimizer(self.lr).minimize(self.loss, global_step=self.global_step)\n",
    "        #self.optimizer = tf.train.GradientDescentOptimizer(self.lr).minimize(self.loss, global_step=self.global_step)        \n",
    "\n",
    "    def _create_summaries(self):\n",
    "        with tf.name_scope('summaries'):\n",
    "            tf.summary.scalar(name='loss', tensor=self.loss)\n",
    "            tf.summary.histogram(name='histogram_loss', values=self.loss)\n",
    "            # merge all summaries into one op to make it easier to manage\n",
    "            self.summary_op = tf.summary.merge_all()\n",
    "\n",
    "    def build_graph(self):\n",
    "        \"\"\" Build the graph for our model \"\"\"\n",
    "        self._create_placeholders()\n",
    "        self._create_embedding()\n",
    "        self._create_loss()\n",
    "        self._create_optimizer()\n",
    "        self._create_summaries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process input and generate batch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(all_articles, vocab_size):\n",
    "    \"\"\" Build vocabulary of VOCAB_SIZE most frequent words \"\"\"\n",
    "    print('Corpus size: {}'.format(len(all_articles)))\n",
    "    words = list(itertools.chain.from_iterable(all_articles)) # flatten all articles into one article\n",
    "    word2id = dict()\n",
    "    word_count = [('UNK', -1)]\n",
    "    word_count.extend(Counter(words).most_common(vocab_size - 1))\n",
    "    print('Total number of unique words: {}'.format(len(Counter(words).most_common())))\n",
    "\n",
    "    idx = 0 # the word index of 'UNK' is now 0    \n",
    "    # save most frequent words for TensorBoard visualization\n",
    "    most_freq_words = codecs.open(filename='processed/vocab_' + str(VOCAB_SIZE) + '.tsv', mode='w', encoding='utf-8')\n",
    "    for word, freq in word_count:\n",
    "        word2id[word] = idx\n",
    "        if idx < VOCAB_SIZE:\n",
    "            most_freq_words.write(word + '\\n')\n",
    "        idx += 1\n",
    "    id2word = dict(zip(word2id.values(), word2id.keys())) # convenient convert word2id to id2word\n",
    "    return word2id, id2word\n",
    "\n",
    "\n",
    "def convert_words_to_index(all_articles, dictionary):\n",
    "    \"\"\" Replace each word in the corpus with its index in the dictionary \"\"\"\n",
    "    word_index_list = []\n",
    "    for each_article in all_articles:\n",
    "        word_index_list.append([dictionary[word] if word in dictionary else 0 for word in each_article]) # if the word index is larger than VOCAB_SIZE, then replace it with 0\n",
    "    return word_index_list\n",
    "\n",
    "def generate_sample(index_words_list, context_window_size, index_dictionary):\n",
    "    \"\"\" Form training pairs according to the skip-gram model. \"\"\"\n",
    "    outer_index = 0\n",
    "    for id_words in itertools.cycle(index_words_list): # infinite loop iterate each article\n",
    "        if DEBUG:\n",
    "            print('================================= outer_index: {}'.format(outer_index))\n",
    "        for inner_index, center in enumerate(id_words): # inside each article\n",
    "            context = random.randint(1, context_window_size)\n",
    "            if DEBUG and outer_index == 0: # print the first article\n",
    "                print('random context:{}'.format(context))\n",
    "            # get a random target before the center word\n",
    "            for target in id_words[max(0, inner_index-context): inner_index]:\n",
    "                if DEBUG and outer_index == 0: # print the first article\n",
    "                    print('before the center word === {} vs {} and {} vs {}'.format(center, target, index_dictionary[center], index_dictionary[target]))\n",
    "                yield center, target\n",
    "            # get a random target after the center word\n",
    "            for target in id_words[(inner_index+1):(inner_index+context+1)]:\n",
    "                if DEBUG and outer_index == 0:\n",
    "                    print('after the center word === {} vs {} and {} vs {}'.format(center, target, index_dictionary[center], index_dictionary[target]))\n",
    "                yield center, target\n",
    "        outer_index += 1\n",
    "\n",
    "def get_batch(iterator, batch_size):\n",
    "    \"\"\" Group a numerical stream into batches and yield them as Numpy arrays. \"\"\"\n",
    "    while True:\n",
    "        center_batch = np.zeros(batch_size, dtype=np.int32)\n",
    "        target_batch = np.zeros([batch_size, 1], dtype=np.int32)\n",
    "        for index in range(batch_size):\n",
    "            center_batch[index], target_batch[index] = next(iterator)\n",
    "        yield center_batch, target_batch\n",
    "\n",
    "def process_data(vocab_size, batch_size, skip_window_size):\n",
    "    all_articles = read_data(file_path=INPUT_FILE)\n",
    "    word2id, id2word = build_vocab(all_articles, vocab_size)\n",
    "    index_words_list = convert_words_to_index(all_articles, word2id)\n",
    "    del all_articles # to save memory\n",
    "    single_gen = generate_sample(index_words_list, skip_window_size, id2word)\n",
    "    return get_batch(single_gen, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_dir(path):\n",
    "    \"\"\" Create a directory if there isn't one already. \"\"\"\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path) # delete an entire directory tree\n",
    "    os.mkdir(path)\n",
    "    \n",
    "def read_data(file_path):\n",
    "    \"\"\" read the input corpus \"\"\"\n",
    "    all_articles = []\n",
    "    for line in codecs.open(filename=file_path, mode='r', encoding='utf-8'):\n",
    "        all_articles.append(line.split())\n",
    "    return all_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, batch_gen, num_train_steps):\n",
    "    \"\"\" start to train the model \"\"\"\n",
    "    saver = tf.train.Saver() # defaults to saving all variables    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint'))\n",
    "        # if that checkpoint exists, restore from checkpoint\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess=sess, save_path=ckpt.model_checkpoint_path)\n",
    "\n",
    "        total_loss = 0.0\n",
    "        writer = tf.summary.FileWriter(logdir='improved_graph/lr' + str(LEARNING_RATE), graph=sess.graph)\n",
    "        initial_step = model.global_step.eval()\n",
    "        for idx in range(initial_step, initial_step+num_train_steps):\n",
    "            centers, targets = next(batch_gen)\n",
    "            #print('Centers shape:{}, targets shape:{}'.format(centers.shape, targets.shape))\n",
    "            feed_dict = {model.center_words: centers, model.target_words: targets}\n",
    "            batch_loss, _, batch_sum = sess.run([model.loss, model.optimizer, model.summary_op], feed_dict=feed_dict)\n",
    "            writer.add_summary(summary=batch_sum, global_step=idx)\n",
    "            total_loss += batch_loss\n",
    "            print('Iteration {}'.format(idx))\n",
    "            if (idx + 1) % SKIP_STEP == 0:           \n",
    "                print('Iteration {} of {}, loss: {:5.3f}'.format(idx, initial_step+num_train_steps, total_loss / SKIP_STEP))\n",
    "                total_loss = 0.0\n",
    "                saver.save(sess=sess, save_path='checkpoints/skip_gram', global_step=idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    make_dir('checkpoints')\n",
    "    make_dir('processed')\n",
    "    make_dir('improved_graph')\n",
    "    \n",
    "    sg_model = SkipGramModel(VOCAB_SIZE, EMBED_SIZE, BATCH_SIZE, NUM_SAMPLED, LEARNING_RATE)\n",
    "    sg_model.build_graph()\n",
    "    batch_gen = process_data(VOCAB_SIZE, BATCH_SIZE, SKIP_WINDOW_SIZE)\n",
    "    train_model(sg_model, batch_gen, NUM_TRAIN_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # hyper-parameters\n",
    "    VOCAB_SIZE = 200000 # the considered size of words\n",
    "    BATCH_SIZE = 128\n",
    "    EMBED_SIZE = 128  # dimension of the word embedding vectors\n",
    "    SKIP_WINDOW_SIZE = 1  # the context window\n",
    "    NUM_SAMPLED = 64  # Number of negative examples to sample.\n",
    "    LEARNING_RATE = 0.5\n",
    "    NUM_TRAIN_STEPS = 1500000\n",
    "    SKIP_STEP = 2000\n",
    "\n",
    "    INPUT_FILE = 'E:/2017_Deep_learning/word2vec/word_vector_108000.cs'\n",
    "\n",
    "    DEBUG = True\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
