{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text classification by Convolution Neural Networks\n",
    "### Model structure and dimensions\n",
    "- Input embedding layer\n",
    "    - `(batch_size, seq_length, embedding_size, 1)`\n",
    "- `for f in filter_sizes`\n",
    "    - Convolution\n",
    "        - Filter shape: `(f, embedding_size, 1, num_filters)`\n",
    "        - Output shape: `(batch_size, seq_length-f+1, 1, num_filters)` with `stride=1`\n",
    "    - Max-pool\n",
    "        - `ksize=[1, seq_length-f+1, 1, 1]`\n",
    "        - Output shape: reshape `(batch_size, 1, 1, num_filters)` to `(batch_size, num_filters)`\n",
    "- Concatenate outputs of all filters together\n",
    "    - `tf.concat(xx, axis=-1)`\n",
    "    - Output shape: `(batch_size, len(filter_sizes)*num_filters)`\n",
    "- FC1 with drop-out\n",
    "    - Output shape: `(batch_size, len(filter_sizes)*num_filters)`\n",
    "- FC2\n",
    "    - Output shape: `(batch_size, num_classes)`\n",
    "\n",
    "### How to add He initialization ?\n",
    "- `he_init = tf.contrib.layers.variance_scaling_initializer()`\n",
    "- `fc1 = tf.contrib.layers.fully_connected(inputs=conv_res, ..., weights_initializer=he_init)\n",
    "\n",
    "### How to perform L2 regularization using advanced API `tf.contrib.layers` ? \n",
    "- `tf.contrib.layers.fully_connected(inputs, num_outputs, weights_regularizer=tf.contrib.layers.l2_regularizer(scale))`\n",
    "- `reg_ws = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)`\n",
    "- `cost = cross_entropy_cost + tf.reduce_sum(reg_ws)` # adds to total cost\n",
    "\n",
    "References:  \n",
    "[1] https://stackoverflow.com/questions/37107223/how-to-add-regularizations-in-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from random import shuffle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class `DataGenerator` is used to read input files, convert words to index and generate batch training or testing data. \n",
    "\n",
    "Since the CNN input has fixed length. `Arguments.MAX_SEQ_LENGTH` records the maximum sequence length in training data. All sequences that lengths less than `Arguments.MAX_SEQ_LENGTH` are padded to `Arguments.MAX_SEQ_LENGTH`.  \n",
    "\n",
    "Two extra words are introduced, `PAD` for padding shorter sequences and `OOV` for representing out-of-vocabulary words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataGenerator():\n",
    "    \"\"\"\n",
    "    reading each training and testing files, and generating batch data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        self.folder_path = args.FOLDER_PATH\n",
    "        self.batch_size = args.BATCH_SIZE\n",
    "        self.vocab_size = args.VOCAB_SIZE\n",
    "        self.num_epoch = args.NUM_EPOCH\n",
    "        self.read_build_input()\n",
    "        self.single_generator_training = self.generate_sample_training()\n",
    "        self.single_generator_testing = self.generate_sample_testing()\n",
    "        self.label_dict = {0:'auto', 1:'business', 2:'IT', 3:'health', 4:'sports', 5:'yule'}\n",
    "    \n",
    "    def one_hot_encode(self, x, n_classes=6):\n",
    "        return np.eye(n_classes)[[x]][0]\n",
    "        \n",
    "    def read_build_input(self):\n",
    "        training_src = []\n",
    "        testing_src = []\n",
    "        article_len = []\n",
    "\n",
    "        for cur_category in range(1, 7):\n",
    "            \n",
    "            print('parsing file >>>>>>>>>>>>>>> ', cur_category)\n",
    "            print('-'*100)\n",
    "            \n",
    "            training_input_file = codecs.open(filename=os.path.join(self.folder_path, 'training_' + str(cur_category) + '.cs'), mode='r', encoding='utf-8')\n",
    "            for tmp_line in training_input_file:\n",
    "                training_src.append((tmp_line.split(), cur_category-1))\n",
    "                article_len.append(len(tmp_line.split()))\n",
    "\n",
    "            testing_input_file = codecs.open(filename=os.path.join(self.folder_path, 'testing_' + str(cur_category) + '.cs'), mode='r', encoding='utf-8')\n",
    "            for tmp_line in testing_input_file:\n",
    "                testing_src.append((tmp_line.split(), cur_category-1))                \n",
    "\n",
    "        shuffle(training_src)\n",
    "        shuffle(testing_src)\n",
    "        \n",
    "        args.MAX_SEQ_LENGTH = max(article_len)\n",
    "        print('='*100)\n",
    "        print('Size of training data:', len(training_src))\n",
    "        print('Size of testing data:', len(testing_src))\n",
    "        print('Maximum length of training articles', args.MAX_SEQ_LENGTH)\n",
    "    \n",
    "        self.TRAINING_SIZE = len(training_src)\n",
    "        args.TESTING_SIZE = len(testing_src)\n",
    "        \n",
    "        training_X_src = [pair[0] for pair in training_src]\n",
    "        testing_X_src = [pair[0] for pair in testing_src]\n",
    "        all_data = list(itertools.chain.from_iterable(training_X_src + testing_X_src))\n",
    "        word_counter = Counter(all_data).most_common(self.vocab_size-2)\n",
    "        del all_data\n",
    "        \n",
    "        print('='*100)\n",
    "        if os.path.isfile(args.MAPFILE):\n",
    "            print('Reload word2idx')\n",
    "            with open(args.MAPFILE, 'rb') as f:\n",
    "                self.word2idx, self.idx2word = pickle.load(f)\n",
    "        else:\n",
    "            print('top 10 frequent words:')\n",
    "            print(word_counter[0:10])\n",
    "            self.word2idx = {val[0]: idx+1 for idx, val in enumerate(word_counter)}\n",
    "            self.word2idx['PAD'] = 0 # padding word\n",
    "            self.word2idx['OOV'] = self.vocab_size - 1 # out-of-vocabulary\n",
    "            self.idx2word = dict(zip(self.word2idx.values(), self.word2idx.keys()))\n",
    "            print('Total vocabulary size:{}'.format(len(self.word2idx)))            \n",
    "        \n",
    "            with open(args.MAPFILE, 'wb') as f:\n",
    "                pickle.dump([self.word2idx, self.idx2word], f)\n",
    "        \n",
    "        self.training = [([self.word2idx[w] if w in self.word2idx else self.word2idx['OOV'] for w in tmp_pair[0][0:args.MAX_SEQ_LENGTH]], tmp_pair[1]) for tmp_pair in training_src]\n",
    "        self.testing_ori =  [([self.word2idx[w] if w in self.word2idx else self.word2idx['OOV'] for w in tmp_pair[0][0:args.MAX_SEQ_LENGTH]], tmp_pair[1]) for tmp_pair in testing_src]\n",
    "        self.testing = [(tmp_pair[0] + [self.word2idx['PAD']] * (args.MAX_SEQ_LENGTH - len(tmp_pair[0])), tmp_pair[1]) if len(tmp_pair[0]) < args.MAX_SEQ_LENGTH else tmp_pair for tmp_pair in self.testing_ori]\n",
    "    \n",
    "    def generate_sample_training(self):\n",
    "        \"\"\"\n",
    "        If len(each article) < self.max_seq_len:\n",
    "            padding them with 0\n",
    "        else:\n",
    "            truncating them to self.max_seq_len\n",
    "        \"\"\"\n",
    "        outer_index = 0\n",
    "        for X_y_pair in itertools.cycle(self.training):  # infinite loop each article\n",
    "            tmp_input_len = len(X_y_pair[0])\n",
    "            if tmp_input_len < args.MAX_SEQ_LENGTH:\n",
    "                input_X = X_y_pair[0] + [self.word2idx['PAD']] * (args.MAX_SEQ_LENGTH - tmp_input_len)\n",
    "            else:\n",
    "                input_X = X_y_pair[0]\n",
    "            \n",
    "            output_y = X_y_pair[1]\n",
    "            if outer_index in [0, 10]:\n",
    "                print('='*100)\n",
    "                print('Training text:', ' '.join([self.idx2word[tmp_id] for tmp_id in input_X]))\n",
    "                print('Training text length:', len(input_X))\n",
    "                print('Training label:', self.label_dict[output_y])\n",
    "                \n",
    "            yield input_X, output_y\n",
    "            outer_index += 1\n",
    "    \n",
    "    def generate_sample_testing(self):\n",
    "        \"\"\"\n",
    "        If len(each article) < self.max_seq_len:\n",
    "            padding them with 0\n",
    "        else:\n",
    "            truncating them to self.max_seq_len\n",
    "        \"\"\"\n",
    "        outer_index = 0\n",
    "        for X_y_pair in self.testing: #itertools.cycle(self.testing):  # infinite loop each article\n",
    "            tmp_input_len = len(X_y_pair[0])\n",
    "            if tmp_input_len < args.MAX_SEQ_LENGTH:\n",
    "                input_X = X_y_pair[0] + [self.word2idx['PAD']] * (args.MAX_SEQ_LENGTH - tmp_input_len)\n",
    "            else:\n",
    "                input_X = X_y_pair[0]\n",
    "            \n",
    "            output_y = X_y_pair[1]\n",
    "            if outer_index in [0, 10]:\n",
    "                print('='*100)\n",
    "                print('Testing text:', ' '.join([self.idx2word[tmp_id] for tmp_id in input_X]))\n",
    "                print('Testing text length:', len(input_X))\n",
    "                print('Testing label:', self.label_dict[output_y])\n",
    "                \n",
    "            yield input_X, output_y\n",
    "            outer_index += 1\n",
    "        \n",
    "\n",
    "    def next_batch_training(self):\n",
    "        input_X_batch = []\n",
    "        output_y_batch = []\n",
    "        for idx in range(self.batch_size):\n",
    "            tmp_X, tmp_y = next(self.single_generator_training)\n",
    "            input_X_batch.append(tmp_X)\n",
    "            output_y_batch.append(self.one_hot_encode(tmp_y))\n",
    "        return np.array(input_X_batch, dtype=np.int32), np.array(output_y_batch, dtype=np.int32)\n",
    "    \n",
    "    def next_testing(self):\n",
    "        testing_X = np.array([tmp_pair[0] for tmp_pair in self.testing], dtype=np.int32)\n",
    "        testing_y = np.array([tmp_pair[1] for tmp_pair in self.testing], dtype=np.int32)\n",
    "        return testing_X, testing_y     \n",
    "    \n",
    "    def testing_data(self):\n",
    "        testing_X = np.array([tmp_pair[0] for tmp_pair in self.testing], dtype=np.int32)\n",
    "        testing_y = np.array([self.one_hot_encode(tmp_pair[1]) for tmp_pair in self.testing], dtype=np.int32)\n",
    "        # here I limit first 10000 samples since my computer memeory limitation\n",
    "        return testing_X[:1000], testing_y[:1000]  \n",
    "        #return testing_X, testing_y  \n",
    "    \n",
    "    \n",
    "    def next_batch_testing(self):\n",
    "        input_X_batch = []\n",
    "        output_y_batch = []\n",
    "        for idx in range(self.batch_size):\n",
    "            tmp_X, tmp_y = next(self.single_generator_testing)\n",
    "            input_X_batch.append(tmp_X)\n",
    "            output_y_batch.append(self.one_hot_encode(tmp_y))\n",
    "        return np.array(input_X_batch, dtype=np.int32), np.array(output_y_batch, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper-parameters for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Arguments:\n",
    "    \"\"\"\n",
    "    main hyper-parameters\n",
    "    \"\"\"\n",
    "    REGULARIZATION = 0.01\n",
    "    EMBED_SIZE = 100 # embedding dimensions\n",
    "    BATCH_SIZE = 32\n",
    "    VOCAB_SIZE = 300000 # vocabulary size\n",
    "    NUM_CLASSES = 6 # number of classes\n",
    "    NUM_FILTERS = 65\n",
    "    # [3, 4, 5] performs best according to http://ruder.io/deep-learning-nlp-best-practices/\n",
    "    FILTER_SIZES = [3, 4, 5]\n",
    "    KEEP_PROB = 0.5\n",
    "    FOLDER_PATH = 'sogou_corpus'\n",
    "    NUM_EPOCH = 10\n",
    "    CHECKPOINTS_DIR = 'text_classification_CNN_model' + '_lambda_' + str(REGULARIZATION)\n",
    "    LOGDIR = 'text_classification_CNN_logdir' + '_lambda_' + str(REGULARIZATION)\n",
    "    MAPFILE = 'cnn_maps.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `conv = tf.nn.conv2d(input, filter, strides, padding, data_format='NHWC')`\n",
    "- `conv` and `input` are both 4-D tensor of shape `(batch_size, in_height, in_width, in_channels)`.\n",
    "- `filter` is a 4-D tensor of shape `(filter_height, filter_width, in_channels, out_channels)`.\n",
    "- `strides` is a list of `ints`, `(batch_stride=1, vertical_stride, horizontal_stride, channels_stride=1)`\n",
    "- `padding`: `SAME` or `VALID`.\n",
    "\n",
    "### `pooled = tf.nn.max_pool(value, ksize, strides, padding, data_format='NHWC')`\n",
    "- `pooled` and `value` are both 4-D tensor of `data_format`.\n",
    "- `ksize` and `strides` are both a list of `ints`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextClassificationModel:\n",
    "    \"\"\"\n",
    "    Model class.\n",
    "    reference: http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/\n",
    "    \"\"\"\n",
    "    def __init__(self, args):\n",
    "        self.l2_reg = args.REGULARIZATION\n",
    "        self.embedding_size = args.EMBED_SIZE\n",
    "        self.batch_size = args.BATCH_SIZE\n",
    "        # for each filter size, how many filters we have\n",
    "        self.num_filters = args.NUM_FILTERS\n",
    "        self.filter_sizes = args.FILTER_SIZES\n",
    "        self.num_classes = args.NUM_CLASSES\n",
    "        self.vocab_size = args.VOCAB_SIZE + 2\n",
    "        self.seq_length = args.MAX_SEQ_LENGTH\n",
    "        self.global_step = tf.Variable(initial_value=0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "        self.best_accuracy = tf.Variable(initial_value=0.0, dtype=tf.float32, trainable=False, name='best_accuracy')\n",
    "        \n",
    "        self.input_X = tf.placeholder(dtype=tf.int32, shape=[None, self.seq_length], name='input_X')\n",
    "        self.output_y = tf.placeholder(dtype=tf.int32, shape = [None, self.num_classes], name='output_y')\n",
    "        # when training, feed keep_prob; when testing, feed 1.0\n",
    "        self.keep_prob = tf.placeholder(dtype=tf.float32, name='keep_prob')\n",
    "        \n",
    "        self.embedding_inputs = self.embedding_layer(self.input_X)\n",
    "        self.conv_res = self.conv_max_pool_layer(self.embedding_inputs)\n",
    "        self.scores, self.predictions, self.accuracy = self.score_layer(self.conv_res)\n",
    "        self.cost = self.cost_layer(self.scores)\n",
    "        self.optimize = self.optimizer(self.cost)\n",
    "        \n",
    "    \n",
    "    def embedding_layer(self, word_inputs):\n",
    "        with tf.variable_scope('word_embedding', initializer=tf.contrib.layers.xavier_initializer()), tf.device('/cpu:0'):\n",
    "            embedding_matrix = tf.get_variable(name='embedding_matrix', shape=[self.vocab_size, self.embedding_size])\n",
    "            inputs = tf.nn.embedding_lookup(params=embedding_matrix, ids=word_inputs, name='embed')\n",
    "            print('inputs shape: ', inputs.get_shape()) # shape: (None, seq_length, embedding_size)\n",
    "            # expand one dimension at last dimension: axis=-1\n",
    "            inputs_expanded = tf.expand_dims(input=inputs, axis=-1)  \n",
    "            print('inputs_expanded shape: ', inputs_expanded.get_shape()) # shape: (None, seq_length, embedding_size, 1)\n",
    "        return inputs_expanded\n",
    "    \n",
    "    def conv_max_pool_layer(self, embed_input):\n",
    "        pooled_outputs = []\n",
    "        for idx, filter_size in enumerate(self.filter_sizes):\n",
    "            with tf.variable_scope('conv-maxpool-%s' % filter_size):\n",
    "                # initialization from https://www.tensorflow.org/programmers_guide/variables\n",
    "                # shape: (filter_height, filter_width, in_channels, out_channels)\n",
    "                filter_W = tf.get_variable(name='filter_W', shape=[filter_size, self.embedding_size, 1, self.num_filters], initializer=tf.random_normal_initializer())\n",
    "                filter_b = tf.get_variable(name='filter_b', shape=[self.num_filters], initializer=tf.constant_initializer(0.0))\n",
    "                conv = tf.nn.conv2d(input=embed_input, filter=filter_W, strides=[1, 1, 1, 1], padding='VALID', name='conv')\n",
    "                conv_activation = tf.nn.relu(tf.nn.bias_add(conv, filter_b), name='relu')\n",
    "                print('filter_size:{}, conv_activation shape:{}'.format(filter_size, conv_activation.get_shape()))\n",
    "                # max-pooling\n",
    "                pooled = tf.nn.max_pool(value=conv_activation, ksize=[1, self.seq_length-filter_size+1, 1, 1], strides=[1, 1, 1, 1], padding='VALID', name='max-pool')\n",
    "                print('filter_size:{}, pooled shape:{}'.format(filter_size, pooled.get_shape()))\n",
    "                pooled_outputs.append(pooled)\n",
    "        conv_pooled = tf.concat(values=pooled_outputs, axis=-1)\n",
    "        print('='*100)\n",
    "        print('conv_pooled shape: ', conv_pooled.get_shape()) # shape: (None, 1, 1, self.num_filters*len(self.filter_sizes))\n",
    "        conv_pooled_flat = tf.reshape(conv_pooled, [-1, self.num_filters*len(self.filter_sizes)]) \n",
    "        print('conv_pooled_flat shape: ', conv_pooled_flat.get_shape()) # shape: (None, self.num_filters*len(self.filter_sizes)\n",
    "        return conv_pooled_flat\n",
    "            \n",
    "    def score_layer(self, conv_res):\n",
    "        with tf.variable_scope('score'):\n",
    "            # fc1 with He initialization\n",
    "            he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "            fc1 = tf.contrib.layers.fully_connected(inputs=conv_res, \n",
    "                                                    num_outputs=self.num_filters*len(self.filter_sizes),\n",
    "                                                    weights_initializer=he_init,\n",
    "                                                    weights_regularizer=tf.contrib.layers.l2_regularizer(scale=self.l2_reg))\n",
    "            print('fc1 shape: ', fc1.get_shape()) # shape: (None, self.num_filters*len(self.filter_sizes))\n",
    "                        \n",
    "            fc1_drop = tf.nn.dropout(fc1, self.keep_prob)\n",
    "            \n",
    "            scores = tf.contrib.layers.fully_connected(inputs=fc1_drop, \n",
    "                                                       num_outputs=self.num_classes, \n",
    "                                                       activation_fn=None,\n",
    "                                                       weights_regularizer=tf.contrib.layers.l2_regularizer(scale=self.l2_reg))\n",
    "            print('scores shape: ', scores.get_shape()) # shape: (None, self.num_classes)\n",
    "            probs = tf.nn.softmax(scores)\n",
    "            predictions = tf.argmax(probs, 1, name='predictions')\n",
    "            print('predictions shape: ', predictions.get_shape()) # shape: (None, )\n",
    "            accuracy = tf.reduce_mean(tf.cast(tf.equal(predictions, tf.argmax(self.output_y, 1)), tf.float32))\n",
    "            tf.summary.scalar(name='accuracy', tensor=accuracy)\n",
    "        return scores, predictions, accuracy\n",
    "        \n",
    "    \n",
    "    def cost_layer(self, scores):\n",
    "        with tf.variable_scope('cost'):\n",
    "            cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=scores, labels=self.output_y))\n",
    "            # https://stackoverflow.com/questions/37107223/how-to-add-regularizations-in-tensorflow\n",
    "            reg_ws = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "            for w in reg_ws:\n",
    "                shp = w.get_shape().as_list()\n",
    "                print('{} shape: {} size: {}'.format(w.name, shp, np.prod(shp)))\n",
    "            cost = cost + tf.reduce_sum(reg_ws)\n",
    "            tf.summary.scalar(name='loss', tensor=cost)\n",
    "            self.summary_op = tf.summary.merge_all()\n",
    "        return cost\n",
    "    \n",
    "    def optimizer(self, cost):\n",
    "        with tf.name_scope('optimizer'):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=1e-3)\n",
    "            optimize = optimizer.minimize(loss=cost, global_step=self.global_step)\n",
    "        return optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train` method to train model.\n",
    "\n",
    "`train_writer` and `test_writer` used as indicator of `accuracy` and `loss` for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(data, model, args):\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        train_writer = tf.summary.FileWriter(logdir=args.LOGDIR + '/train', graph=sess.graph)\n",
    "        test_writer = tf.summary.FileWriter(logdir=args.LOGDIR + '/test')\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        ckpt = tf.train.get_checkpoint_state(checkpoint_dir=args.CHECKPOINTS_DIR)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess=sess, save_path=ckpt.model_checkpoint_path)\n",
    "            print(ckpt)\n",
    "        \n",
    "        max_iteration_num = args.NUM_EPOCH * data.TRAINING_SIZE // args.BATCH_SIZE        \n",
    "        for idx in range(1, max_iteration_num):\n",
    "            batch_X, batch_y = data.next_batch_training()\n",
    "            \n",
    "            feed_dict = {model.input_X: batch_X, model.output_y: batch_y, model.keep_prob: args.KEEP_PROB}\n",
    "            tmp_accuracy, tmp_cost, _, tmp_summary = sess.run([model.accuracy, model.cost, model.optimize, model.summary_op], feed_dict=feed_dict)\n",
    "            train_writer.add_summary(summary=tmp_summary, global_step=model.global_step.eval())\n",
    "            \n",
    "            if idx % 50 == 0:\n",
    "                print('='*100)\n",
    "                print('Step: {} / {}, training loss:{:4f}, training accuracy:{:4f}'.format(idx, max_iteration_num, tmp_cost, tmp_accuracy))\n",
    "                \n",
    "            if idx % 200 == 0:\n",
    "                test_cost = 0.0\n",
    "                test_accuracy = 0.0\n",
    "                cc = 0\n",
    "                test_batch_X, test_batch_y = data.testing_data()\n",
    "                test_feed_dict = {model.input_X: test_batch_X, model.output_y: test_batch_y, model.keep_prob:1.0}\n",
    "                test_tmp_cost, test_tmp_accuracy, test_tmp_summary = sess.run([model.cost, model.accuracy, model.summary_op], feed_dict=test_feed_dict)\n",
    "                test_writer.add_summary(summary=test_tmp_summary, global_step=model.global_step.eval())\n",
    "                print('-'*100)\n",
    "                #print('Step:{}, testing accuracy:{:4f}'.format(model.global_step.eval(), test_tmp_accuracy))\n",
    "                print('Step: {} / {}, testing loss:{:4f}, testing accuracy:{:4f}'.format(idx, max_iteration_num, test_tmp_cost, test_tmp_accuracy))\n",
    "                if test_tmp_accuracy > model.best_accuracy.eval():\n",
    "                    print('Best model accuracy: {:4f}'.format(test_tmp_accuracy))\n",
    "                    sess.run(model.best_accuracy.assign(test_tmp_accuracy))\n",
    "                    saver.save(sess=sess, save_path=os.path.join(args.CHECKPOINTS_DIR, 'text_classification_cnn.ckpt'), global_step=model.global_step.eval())\n",
    "                else:                    \n",
    "                    print('Best model accuracy: {:4f}, Current model accuracy: {:4f}'.format(model.best_accuracy.eval(), test_tmp_accuracy))\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`test` method to load the trained model and test model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(data, model, args):\n",
    "    categories = ['auto', 'business', 'IT', 'health', 'sports', 'yule']\n",
    "    saver = tf.train.Saver()    \n",
    "    with tf.Session() as sess:\n",
    "        ckpt = tf.train.get_checkpoint_state(checkpoint_dir=args.CHECKPOINTS_DIR)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess=sess, save_path=ckpt.model_checkpoint_path)\n",
    "            print(ckpt)\n",
    "        test_X, test_y = data.testing_data()\n",
    "        print('test_X shape: ', test_X.shape)\n",
    "        print('test_y shape: ', test_y.shape)\n",
    "        test_feed_dict = {model.input_X: test_X, model.output_y:test_y, model.keep_prob:1.0}\n",
    "        test_predictions = sess.run(model.predictions, feed_dict=test_feed_dict)\n",
    "        print('Precision, Recall and F1-Score:')\n",
    "        test_y = np.argmax(test_y, 1)\n",
    "        print(metrics.classification_report(test_y, test_predictions, target_names=categories))\n",
    "\n",
    "        print('Confusion matrix:')\n",
    "        cm = metrics.confusion_matrix(test_y, test_predictions)\n",
    "        print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    args = Arguments()\n",
    "    data = DataGenerator(args)\n",
    "    model = TextClassificationModel(args)\n",
    "    \n",
    "    # for training\n",
    "    #train(data, model, args)   \n",
    "    \n",
    "    # testing\n",
    "    test(data, model, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
